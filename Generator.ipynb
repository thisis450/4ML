{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# 定义氨基酸字母表和映射\n",
    "AMINO_ACID_ALPHABET = 'ACDEFGHIKLMNPQRSTVWY-'  # 20个标准氨基酸\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AMINO_ACID_ALPHABET)}  # 字符到索引的映射\n",
    "IDX_TO_AA = {i: aa for i, aa in enumerate(AMINO_ACID_ALPHABET)}  # 索引到字符的映射\n",
    "END_IDX = len(AMINO_ACID_ALPHABET)  # 结束符的特殊索引\n",
    "ALL_IDX = list(AA_TO_IDX.values()) + [END_IDX]  # 包含结束符的索引\n",
    "# 数据集文件路径\n",
    "data_file_path = './MSA_nat_with_annotation.faa'\n",
    "\n",
    "# 读取并处理蛋白质序列\n",
    "def load_protein_data(file_path):\n",
    "    sequences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines), 2):  # 每两个行一组\n",
    "            sequence = lines[i + 1].strip()  # 获取蛋白质序列\n",
    "            # 合并多行蛋白质序列（处理换行符）\n",
    "            full_sequence = ''.join(sequence.split())  # 移除所有换行符\n",
    "            sequences.append(full_sequence)\n",
    "    return sequences\n",
    "\n",
    "# 将氨基酸序列转换为模型输入的张量\n",
    "def amino_acid_to_tensor(sequence):\n",
    "    \"\"\"将氨基酸序列转换为模型输入的张量\"\"\"\n",
    "    tensor = torch.tensor([AA_TO_IDX.get(aa, END_IDX) for aa in sequence])  # 如果是 gap 使用 END_IDX\n",
    "    return tensor.unsqueeze(0)  # 返回一个 batch 的维度\n",
    "\n",
    "# 加载并处理数据\n",
    "protein_sequences = load_protein_data(data_file_path)\n",
    "\n",
    "# 将所有蛋白质序列转换为张量\n",
    "sequence_tensors = [amino_acid_to_tensor(seq) for seq in protein_sequences]\n",
    "\n",
    "# 打印前几个样本的长度和内容\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"Sequence Length: {sequence_tensors[i].size(1)}\")\n",
    "    print(f\"Sequence (first 30 chars): {protein_sequences[i][:30]}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], D Loss: 1.405148983001709, G Loss: 0.7492223978042603\n",
      "Epoch [100/10000], D Loss: 0.12468011677265167, G Loss: 2.451477527618408\n",
      "Epoch [200/10000], D Loss: 0.025469431653618813, G Loss: 4.315186977386475\n",
      "Epoch [300/10000], D Loss: 0.013223692774772644, G Loss: 5.083759307861328\n",
      "Epoch [400/10000], D Loss: 0.008859571069478989, G Loss: 5.484199047088623\n",
      "Epoch [500/10000], D Loss: 0.005743398796766996, G Loss: 6.062453746795654\n",
      "Epoch [600/10000], D Loss: 0.004094420000910759, G Loss: 6.516589164733887\n",
      "Epoch [700/10000], D Loss: 0.003776867873966694, G Loss: 6.3800883293151855\n",
      "Epoch [800/10000], D Loss: 0.002558184787631035, G Loss: 7.05155086517334\n",
      "Epoch [900/10000], D Loss: 0.0021740912925451994, G Loss: 7.173276901245117\n",
      "Epoch [1000/10000], D Loss: 0.002008459996432066, G Loss: 7.104085445404053\n",
      "Epoch [1100/10000], D Loss: 0.0017811296274885535, G Loss: 7.175820827484131\n",
      "Epoch [1200/10000], D Loss: 0.0015073753893375397, G Loss: 7.370362758636475\n",
      "Epoch [1300/10000], D Loss: 0.0013370502274483442, G Loss: 7.46561336517334\n",
      "Epoch [1400/10000], D Loss: 0.0011217815335839987, G Loss: 7.703991413116455\n",
      "Epoch [1500/10000], D Loss: 0.0009735675994306803, G Loss: 7.8721160888671875\n",
      "Epoch [1600/10000], D Loss: 0.0008716426091268659, G Loss: 7.971149921417236\n",
      "Epoch [1700/10000], D Loss: 0.0008202607277780771, G Loss: 7.959928512573242\n",
      "Epoch [1800/10000], D Loss: 0.000705813174135983, G Loss: 8.171828269958496\n",
      "Epoch [1900/10000], D Loss: 0.0006023326423019171, G Loss: 8.41681957244873\n",
      "Epoch [2000/10000], D Loss: 0.000607049441896379, G Loss: 8.248239517211914\n",
      "Epoch [2100/10000], D Loss: 0.0005154235404916108, G Loss: 8.510300636291504\n",
      "Epoch [2200/10000], D Loss: 0.000423810794018209, G Loss: 8.89289665222168\n",
      "Epoch [2300/10000], D Loss: 0.00043594551971182227, G Loss: 8.65650749206543\n",
      "Epoch [2400/10000], D Loss: 0.0003714343183673918, G Loss: 8.93842887878418\n",
      "Epoch [2500/10000], D Loss: 0.00039267295505851507, G Loss: 8.667732238769531\n",
      "Epoch [2600/10000], D Loss: 0.0003350381157360971, G Loss: 8.93412971496582\n",
      "Epoch [2700/10000], D Loss: 0.00032125692814588547, G Loss: 8.920345306396484\n",
      "Epoch [2800/10000], D Loss: 0.0002897961821872741, G Loss: 9.05842399597168\n",
      "Epoch [2900/10000], D Loss: 0.00027012452483177185, G Loss: 9.121373176574707\n",
      "Epoch [3000/10000], D Loss: 0.00023556672385893762, G Loss: 9.35852336883545\n",
      "Epoch [3100/10000], D Loss: 0.00023768952814862132, G Loss: 9.221052169799805\n",
      "Epoch [3200/10000], D Loss: 0.0002272215933771804, G Loss: 9.22913646697998\n",
      "Epoch [3300/10000], D Loss: 0.00017711684631649405, G Loss: 9.77191162109375\n",
      "Epoch [3400/10000], D Loss: 0.00017697922885417938, G Loss: 9.641291618347168\n",
      "Epoch [3500/10000], D Loss: 0.00018375084619037807, G Loss: 9.442984580993652\n",
      "Epoch [3600/10000], D Loss: 0.00017045947606675327, G Loss: 9.526755332946777\n",
      "Epoch [3700/10000], D Loss: 0.00015136061119847, G Loss: 9.719314575195312\n",
      "Epoch [3800/10000], D Loss: 0.0001486601249780506, G Loss: 9.668328285217285\n",
      "Epoch [3900/10000], D Loss: 0.00014083433779887855, G Loss: 9.706212997436523\n",
      "Epoch [4000/10000], D Loss: 0.00011880222882609814, G Loss: 10.036203384399414\n",
      "Epoch [4100/10000], D Loss: 0.00011625637125689536, G Loss: 9.989374160766602\n",
      "Epoch [4200/10000], D Loss: 0.0001037820620695129, G Loss: 10.18552017211914\n",
      "Epoch [4300/10000], D Loss: 0.00010304775059921667, G Loss: 10.101240158081055\n",
      "Epoch [4400/10000], D Loss: 0.00010582842514850199, G Loss: 9.95252513885498\n",
      "Epoch [4500/10000], D Loss: 9.273162868339568e-05, G Loss: 10.178388595581055\n",
      "Epoch [4600/10000], D Loss: 7.87489116191864e-05, G Loss: 10.506678581237793\n",
      "Epoch [4700/10000], D Loss: 7.97709944890812e-05, G Loss: 10.370381355285645\n",
      "Epoch [4800/10000], D Loss: 8.28236443339847e-05, G Loss: 10.19776439666748\n",
      "Epoch [4900/10000], D Loss: 6.937189027667046e-05, G Loss: 10.540732383728027\n",
      "Epoch [5000/10000], D Loss: 6.446205225074664e-05, G Loss: 10.64100170135498\n",
      "Epoch [5100/10000], D Loss: 5.8640489442041144e-05, G Loss: 10.79594898223877\n",
      "Epoch [5200/10000], D Loss: 5.862489342689514e-05, G Loss: 10.697141647338867\n",
      "Epoch [5300/10000], D Loss: 5.376886474550702e-05, G Loss: 10.830759048461914\n",
      "Epoch [5400/10000], D Loss: 4.2602201574482024e-05, G Loss: 11.461310386657715\n",
      "Epoch [5500/10000], D Loss: 4.781132884090766e-05, G Loss: 10.952086448669434\n",
      "Epoch [5600/10000], D Loss: 4.767344944411889e-05, G Loss: 10.86845588684082\n",
      "Epoch [5700/10000], D Loss: 4.1524097468936816e-05, G Loss: 11.144351959228516\n",
      "Epoch [5800/10000], D Loss: 4.1364306525792927e-05, G Loss: 11.06020736694336\n",
      "Epoch [5900/10000], D Loss: 4.3659561924869195e-05, G Loss: 10.847887992858887\n",
      "Epoch [6000/10000], D Loss: 3.676794221973978e-05, G Loss: 11.185721397399902\n",
      "Epoch [6100/10000], D Loss: 3.2332267437595874e-05, G Loss: 11.45177936553955\n",
      "Epoch [6200/10000], D Loss: 3.2943105907179415e-05, G Loss: 11.295260429382324\n",
      "Epoch [6300/10000], D Loss: 3.0444305593846366e-05, G Loss: 11.416959762573242\n",
      "Epoch [6400/10000], D Loss: 3.2403604564024135e-05, G Loss: 11.173999786376953\n",
      "Epoch [6500/10000], D Loss: 3.0354960472323e-05, G Loss: 11.255097389221191\n",
      "Epoch [6600/10000], D Loss: 2.921968189184554e-05, G Loss: 11.26921272277832\n",
      "Epoch [6700/10000], D Loss: 2.7277483241050504e-05, G Loss: 11.359969139099121\n",
      "Epoch [6800/10000], D Loss: 2.4778113584034145e-05, G Loss: 11.514019012451172\n",
      "Epoch [6900/10000], D Loss: 1.8792063201544806e-05, G Loss: 12.263031959533691\n",
      "Epoch [7000/10000], D Loss: 2.1270938304951414e-05, G Loss: 11.746766090393066\n",
      "Epoch [7100/10000], D Loss: 2.0328712707851082e-05, G Loss: 11.775788307189941\n",
      "Epoch [7200/10000], D Loss: 1.9244544091634452e-05, G Loss: 11.825002670288086\n",
      "Epoch [7300/10000], D Loss: 1.7055866919690743e-05, G Loss: 12.070098876953125\n",
      "Epoch [7400/10000], D Loss: 1.905213139252737e-05, G Loss: 11.697064399719238\n",
      "Epoch [7500/10000], D Loss: 1.7878786820801906e-05, G Loss: 11.784613609313965\n",
      "Epoch [7600/10000], D Loss: 1.7034188203979284e-05, G Loss: 11.817597389221191\n",
      "Epoch [7700/10000], D Loss: 1.4520882359647658e-05, G Loss: 12.14039421081543\n",
      "Epoch [7800/10000], D Loss: 1.4144868146104272e-05, G Loss: 12.121783256530762\n",
      "Epoch [7900/10000], D Loss: 1.5156399967963807e-05, G Loss: 11.880356788635254\n",
      "Epoch [8000/10000], D Loss: 1.059851001627976e-05, G Loss: 12.811410903930664\n",
      "Epoch [8100/10000], D Loss: 1.2996508303331211e-05, G Loss: 12.113954544067383\n",
      "Epoch [8200/10000], D Loss: 1.1835924851766322e-05, G Loss: 12.246922492980957\n",
      "Epoch [8300/10000], D Loss: 1.2223871635796968e-05, G Loss: 12.102745056152344\n",
      "Epoch [8400/10000], D Loss: 9.647942533774767e-06, G Loss: 12.649530410766602\n",
      "Epoch [8500/10000], D Loss: 9.817023055802565e-06, G Loss: 12.497692108154297\n",
      "Epoch [8600/10000], D Loss: 9.844148735282943e-06, G Loss: 12.39980697631836\n",
      "Epoch [8700/10000], D Loss: 8.85433291841764e-06, G Loss: 12.600922584533691\n",
      "Epoch [8800/10000], D Loss: 8.794810128165409e-06, G Loss: 12.549291610717773\n",
      "Epoch [8900/10000], D Loss: 7.4036488513229415e-06, G Loss: 12.893243789672852\n",
      "Epoch [9000/10000], D Loss: 6.290752480708761e-06, G Loss: 13.320256233215332\n",
      "Epoch [9100/10000], D Loss: 8.211125532398e-06, G Loss: 12.481216430664062\n",
      "Epoch [9200/10000], D Loss: 7.004229246376781e-06, G Loss: 12.77515697479248\n",
      "Epoch [9300/10000], D Loss: 6.564220711879898e-06, G Loss: 12.895261764526367\n",
      "Epoch [9400/10000], D Loss: 6.1235768953338265e-06, G Loss: 12.97926139831543\n",
      "Epoch [9500/10000], D Loss: 5.725501068809535e-06, G Loss: 13.050922393798828\n",
      "Epoch [9600/10000], D Loss: 5.745195721829077e-06, G Loss: 12.988327980041504\n",
      "Epoch [9700/10000], D Loss: 5.7386941989534535e-06, G Loss: 12.891898155212402\n",
      "Epoch [9800/10000], D Loss: 4.914891178486869e-06, G Loss: 13.219673156738281\n",
      "Epoch [9900/10000], D Loss: 4.779177743330365e-06, G Loss: 13.22883415222168\n",
      "Generated Protein Sequence: MEQNYCCCNH-GKQACKCF-MSQPGEAKPVND-THFSPMC-VRP-PRCPLEIDHVRHLKYTINIQCYHIQIT-PYYVKWMFNWYAFDFFYTWTLHQRNQQ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义生成器（Generator）模型\n",
    "class ProteinSequenceGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, max_seq_len):\n",
    "        super(ProteinSequenceGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # 输入维度和嵌入维度\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 输出为氨基酸的数量（包括结束符）\n",
    "\n",
    "        self.max_seq_len = max_seq_len  # 生成序列的最大长度\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 将输入序列映射到低维空间\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
    "        out = self.fc(lstm_out)  # out shape: [batch_size, seq_len, output_dim]\n",
    "        return out\n",
    "\n",
    "    def generate_sequence(self, seed, max_seq_len):\n",
    "        \"\"\"\n",
    "        生成蛋白质序列\n",
    "        seed: 初始输入（通常是一个随机的氨基酸索引）\n",
    "        max_seq_len: 生成序列的最大长度\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            generated_sequence = seed  # 初始种子\n",
    "            for _ in range(max_seq_len - seed.size(1)):  # 生成最大序列长度\n",
    "                output = self.forward(generated_sequence)\n",
    "                next_idx = torch.argmax(output[:, -1, :], dim=-1)  # 选择概率最大的氨基酸\n",
    "                next_one_hot = torch.zeros(generated_sequence.size(0), 1, len(ALL_IDX)).scatter_(2, next_idx.unsqueeze(-1).unsqueeze(1), 1)\n",
    "                \n",
    "                # 调整维度使得可以拼接\n",
    "                generated_sequence = generated_sequence.unsqueeze(2)  # 将 generated_sequence 扩展为三维张量 [batch_size, seq_len, 1]\n",
    "                generated_sequence = torch.cat((generated_sequence, next_one_hot), dim=1)  # 在序列的末尾拼接 next_one_hot\n",
    "\n",
    "                # 如果生成的序列包含了 END_IDX，则停止生成\n",
    "                if next_idx.item() == END_IDX:\n",
    "                    break\n",
    "\n",
    "            return generated_sequence\n",
    "\n",
    "# 定义判别器（Discriminator）模型\n",
    "class ProteinSequenceDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(ProteinSequenceDiscriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # 输入维度和嵌入维度\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 输出为真假判断\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 将输入序列映射到低维空间\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
    "        out = self.fc(lstm_out[:, -1, :])  # 只取最后一层输出进行分类\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# 定义生成器和判别器的优化器和损失函数\n",
    "generator = ProteinSequenceGenerator(len(ALL_IDX), embedding_dim=64, hidden_dim=128, output_dim=len(ALL_IDX), max_seq_len=100)\n",
    "discriminator = ProteinSequenceDiscriminator(len(ALL_IDX), embedding_dim=64, hidden_dim=128)\n",
    "\n",
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# 生成一个种子序列并将其转换为张量\n",
    "def amino_acid_to_tensor(sequence):\n",
    "    \"\"\"将氨基酸序列转换为模型输入的张量\"\"\"\n",
    "    tensor = torch.tensor([AA_TO_IDX.get(aa, END_IDX) for aa in sequence])  # 如果是 gap 使用 END_IDX\n",
    "    return tensor.unsqueeze(0)  # 返回一个 batch 的维度\n",
    "\n",
    "# 示例：训练生成器和判别器\n",
    "for epoch in range(10000):\n",
    "    # 生成真实数据（用一些实际的蛋白质序列）\n",
    "    real_sequence = amino_acid_to_tensor(\"M\" * 100)  # 示例的真实序列，假设有 100 个氨基酸\n",
    "    batch_size = real_sequence.size(0)\n",
    "    \n",
    "    # 生成假的数据（使用生成器）\n",
    "    noise = torch.randint(0, len(ALL_IDX), (batch_size, 100)).long()  # 随机噪声，种子为随机索引\n",
    "    fake_sequence = generator.generate_sequence(noise, max_seq_len=100)\n",
    "\n",
    "    # 判别器训练\n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    real_output = discriminator(real_sequence)\n",
    "    fake_output = discriminator(fake_sequence.detach())  # 不计算梯度\n",
    "    \n",
    "    real_label = torch.ones(batch_size, 1)\n",
    "    fake_label = torch.zeros(batch_size, 1)\n",
    "    \n",
    "    real_loss = criterion(real_output, real_label)\n",
    "    fake_loss = criterion(fake_output, fake_label)\n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "\n",
    "    # 生成器训练\n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    output = discriminator(fake_sequence)\n",
    "    g_loss = criterion(output, real_label)  # 目标是让生成的序列被判别器判为真实\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/10000], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "# 示例：生成一个蛋白质序列\n",
    "noise = torch.randint(0, len(ALL_IDX), (1, 100)).long()  # 随机生成一个种子\n",
    "generated_sequence = generator.generate_sequence(noise, max_seq_len=100)\n",
    "\n",
    "# 将生成的张量转换回氨基酸序列\n",
    "def tensor_to_amino_acid(tensor):\n",
    "    return ''.join([IDX_TO_AA[idx.item()] if idx.item() < len(AMINO_ACID_ALPHABET) else '-' for idx in tensor.squeeze()])\n",
    "\n",
    "generated_protein_sequence = tensor_to_amino_acid(generated_sequence)\n",
    "print(f\"Generated Protein Sequence: {generated_protein_sequence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
