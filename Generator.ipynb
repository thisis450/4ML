{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Sequence Length: 70\n",
      "Sequence (first 30 chars): -TSENPLLALREKISALDEKLLALLAERRE\n",
      "==================================================\n",
      "Sample 2:\n",
      "Sequence Length: 0\n",
      "Sequence (first 30 chars): \n",
      "==================================================\n",
      "Sample 3:\n",
      "Sequence Length: 70\n",
      "Sequence (first 30 chars): ---DERIQALRKEVDRVNREILRLLSERGR\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "# 定义氨基酸字母表和映射\n",
    "AMINO_ACID_ALPHABET = 'ACDEFGHIKLMNPQRSTVWY-'  # 20个标准氨基酸\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AMINO_ACID_ALPHABET)}  # 字符到索引的映射\n",
    "IDX_TO_AA = {i: aa for i, aa in enumerate(AMINO_ACID_ALPHABET)}  # 索引到字符的映射\n",
    "END_IDX = len(AMINO_ACID_ALPHABET)  # 结束符的特殊索引\n",
    "ALL_IDX = list(AA_TO_IDX.values()) + [END_IDX]  # 包含结束符的索引\n",
    "# 数据集文件路径\n",
    "data_file_path = './MSA_nat_with_annotation.faa'\n",
    "\n",
    "# 读取并处理蛋白质序列\n",
    "def load_protein_data(file_path):\n",
    "    sequences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i in range(0, len(lines), 2):  # 每两个行一组\n",
    "            sequence = lines[i + 1].strip()  # 获取蛋白质序列\n",
    "            # 合并多行蛋白质序列（处理换行符）\n",
    "            full_sequence = ''.join(sequence.split())  # 移除所有换行符\n",
    "            sequences.append(full_sequence)\n",
    "    return sequences\n",
    "\n",
    "# 将氨基酸序列转换为模型输入的张量\n",
    "def amino_acid_to_tensor(sequence):\n",
    "    \"\"\"将氨基酸序列转换为模型输入的张量\"\"\"\n",
    "    tensor = torch.tensor([AA_TO_IDX.get(aa, END_IDX) for aa in sequence])  # 如果是 gap 使用 END_IDX\n",
    "    return tensor.unsqueeze(0)  # 返回一个 batch 的维度\n",
    "\n",
    "# 加载并处理数据\n",
    "protein_sequences = load_protein_data(data_file_path)\n",
    "\n",
    "# 将所有蛋白质序列转换为张量\n",
    "sequence_tensors = [amino_acid_to_tensor(seq) for seq in protein_sequences]\n",
    "\n",
    "# 打印前几个样本的长度和内容\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"Sequence Length: {sequence_tensors[i].size(1)}\")\n",
    "    print(f\"Sequence (first 30 chars): {protein_sequences[i][:30]}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], D Loss: 1.3744752407073975, G Loss: 0.680109977722168\n",
      "Epoch [100/10000], D Loss: 0.11042282730340958, G Loss: 2.522927761077881\n",
      "Epoch [200/10000], D Loss: 0.02322295680642128, G Loss: 4.334600448608398\n",
      "Epoch [300/10000], D Loss: 0.010925592854619026, G Loss: 5.25001859664917\n",
      "Epoch [400/10000], D Loss: 0.0069192396476864815, G Loss: 5.75496244430542\n",
      "Epoch [500/10000], D Loss: 0.005159013904631138, G Loss: 6.011691570281982\n",
      "Epoch [600/10000], D Loss: 0.003916616551578045, G Loss: 6.291225433349609\n",
      "Epoch [700/10000], D Loss: 0.0030743200331926346, G Loss: 6.540650367736816\n",
      "Epoch [800/10000], D Loss: 0.0024832719936966896, G Loss: 6.760374546051025\n",
      "Epoch [900/10000], D Loss: 0.0019082537619397044, G Loss: 7.116319179534912\n",
      "Epoch [1000/10000], D Loss: 0.0017164727905765176, G Loss: 7.145064353942871\n",
      "Epoch [1100/10000], D Loss: 0.0014922369737178087, G Loss: 7.266745090484619\n",
      "Epoch [1200/10000], D Loss: 0.0011775586754083633, G Loss: 7.619675159454346\n",
      "Epoch [1300/10000], D Loss: 0.0011075539514422417, G Loss: 7.587809085845947\n",
      "Epoch [1400/10000], D Loss: 0.0009259225334972143, G Loss: 7.833983421325684\n",
      "Epoch [1500/10000], D Loss: 0.0008724287617951632, G Loss: 7.819178104400635\n",
      "Epoch [1600/10000], D Loss: 0.0007367028156295419, G Loss: 8.057100296020508\n",
      "Epoch [1700/10000], D Loss: 0.0006860860739834607, G Loss: 8.081854820251465\n",
      "Epoch [1800/10000], D Loss: 0.0006343527929857373, G Loss: 8.131975173950195\n",
      "Epoch [1900/10000], D Loss: 0.0005676713190041482, G Loss: 8.258238792419434\n",
      "Epoch [2000/10000], D Loss: 0.00048530171625316143, G Loss: 8.494465827941895\n",
      "Epoch [2100/10000], D Loss: 0.0004270771169103682, G Loss: 8.674492835998535\n",
      "Epoch [2200/10000], D Loss: 0.0003978834138251841, G Loss: 8.720137596130371\n",
      "Epoch [2300/10000], D Loss: 0.0003939572488889098, G Loss: 8.632296562194824\n",
      "Epoch [2400/10000], D Loss: 0.00036774075124412775, G Loss: 8.68501091003418\n",
      "Epoch [2500/10000], D Loss: 0.0003144894144497812, G Loss: 8.938905715942383\n",
      "Epoch [2600/10000], D Loss: 0.00029452453600242734, G Loss: 8.989178657531738\n",
      "Epoch [2700/10000], D Loss: 0.0002807108103297651, G Loss: 9.000624656677246\n",
      "Epoch [2800/10000], D Loss: 0.0002645454369485378, G Loss: 9.040571212768555\n",
      "Epoch [2900/10000], D Loss: 0.00024596982984803617, G Loss: 9.114354133605957\n",
      "Epoch [3000/10000], D Loss: 0.00023777358001098037, G Loss: 9.103382110595703\n",
      "Epoch [3100/10000], D Loss: 0.0002189826191170141, G Loss: 9.200611114501953\n",
      "Epoch [3200/10000], D Loss: 0.00018604738579597324, G Loss: 9.486961364746094\n",
      "Epoch [3300/10000], D Loss: 0.00017831815057434142, G Loss: 9.494545936584473\n",
      "Epoch [3400/10000], D Loss: 0.0001691875368123874, G Loss: 9.527776718139648\n",
      "Epoch [3500/10000], D Loss: 0.00016439227329101413, G Loss: 9.51032543182373\n",
      "Epoch [3600/10000], D Loss: 0.0001477622427046299, G Loss: 9.669352531433105\n",
      "Epoch [3700/10000], D Loss: 0.0001133112091338262, G Loss: 10.292232513427734\n",
      "Epoch [3800/10000], D Loss: 0.00012989631795790046, G Loss: 9.80189323425293\n",
      "Epoch [3900/10000], D Loss: 0.00011474320490378886, G Loss: 10.014762878417969\n",
      "Epoch [4000/10000], D Loss: 0.00011444328993093222, G Loss: 9.931784629821777\n",
      "Epoch [4100/10000], D Loss: 0.00011048080341424793, G Loss: 9.932496070861816\n",
      "Epoch [4200/10000], D Loss: 0.00010340036533307284, G Loss: 10.0072660446167\n",
      "Epoch [4300/10000], D Loss: 9.942685574060306e-05, G Loss: 10.021474838256836\n",
      "Epoch [4400/10000], D Loss: 8.327321847900748e-05, G Loss: 10.36506462097168\n",
      "Epoch [4500/10000], D Loss: 6.921648309798911e-05, G Loss: 10.803489685058594\n",
      "Epoch [4600/10000], D Loss: 8.231276297010481e-05, G Loss: 10.224555969238281\n",
      "Epoch [4700/10000], D Loss: 7.634576468262821e-05, G Loss: 10.324766159057617\n",
      "Epoch [4800/10000], D Loss: 7.20803436706774e-05, G Loss: 10.383767127990723\n",
      "Epoch [4900/10000], D Loss: 6.694976764265448e-05, G Loss: 10.476607322692871\n",
      "Epoch [5000/10000], D Loss: 6.133691931609064e-05, G Loss: 10.612554550170898\n",
      "Epoch [5100/10000], D Loss: 5.6922512158052996e-05, G Loss: 10.71456527709961\n",
      "Epoch [5200/10000], D Loss: 5.692846025340259e-05, G Loss: 10.632047653198242\n",
      "Epoch [5300/10000], D Loss: 5.488200986292213e-05, G Loss: 10.642864227294922\n",
      "Epoch [5400/10000], D Loss: 4.952630843035877e-05, G Loss: 10.810425758361816\n",
      "Epoch [5500/10000], D Loss: 5.037206574343145e-05, G Loss: 10.698429107666016\n",
      "Epoch [5600/10000], D Loss: 4.7692326916148886e-05, G Loss: 10.75537109375\n",
      "Epoch [5700/10000], D Loss: 4.045990135637112e-05, G Loss: 11.072309494018555\n",
      "Epoch [5800/10000], D Loss: 4.186023579677567e-05, G Loss: 10.911669731140137\n",
      "Epoch [5900/10000], D Loss: 3.553671558620408e-05, G Loss: 11.241549491882324\n",
      "Epoch [6000/10000], D Loss: 2.9937624276499264e-05, G Loss: 11.650599479675293\n",
      "Epoch [6100/10000], D Loss: 3.321601980132982e-05, G Loss: 11.245843887329102\n",
      "Epoch [6200/10000], D Loss: 3.306390135549009e-05, G Loss: 11.177779197692871\n",
      "Epoch [6300/10000], D Loss: 3.0393128326977603e-05, G Loss: 11.308730125427246\n",
      "Epoch [6400/10000], D Loss: 3.060575545532629e-05, G Loss: 11.218008041381836\n",
      "Epoch [6500/10000], D Loss: 2.6397894544061273e-05, G Loss: 11.506925582885742\n",
      "Epoch [6600/10000], D Loss: 2.167954335163813e-05, G Loss: 11.994309425354004\n",
      "Epoch [6700/10000], D Loss: 2.482007403159514e-05, G Loss: 11.497953414916992\n",
      "Epoch [6800/10000], D Loss: 2.3229131329571828e-05, G Loss: 11.588071823120117\n",
      "Epoch [6900/10000], D Loss: 2.307174690940883e-05, G Loss: 11.529797554016113\n",
      "Epoch [7000/10000], D Loss: 2.128278538293671e-05, G Loss: 11.645249366760254\n",
      "Epoch [7100/10000], D Loss: 1.9165432604495436e-05, G Loss: 11.835808753967285\n",
      "Epoch [7200/10000], D Loss: 1.9109289496554993e-05, G Loss: 11.763997077941895\n",
      "Epoch [7300/10000], D Loss: 1.8254469978273846e-05, G Loss: 11.797822952270508\n",
      "Epoch [7400/10000], D Loss: 1.6981623048195615e-05, G Loss: 11.909601211547852\n",
      "Epoch [7500/10000], D Loss: 1.6968642739811912e-05, G Loss: 11.826565742492676\n",
      "Epoch [7600/10000], D Loss: 1.5164883734541945e-05, G Loss: 12.026693344116211\n",
      "Epoch [7700/10000], D Loss: 1.5548814189969562e-05, G Loss: 11.909950256347656\n",
      "Epoch [7800/10000], D Loss: 1.4606853255827446e-05, G Loss: 11.981608390808105\n",
      "Epoch [7900/10000], D Loss: 1.3662023775395937e-05, G Loss: 12.059321403503418\n",
      "Epoch [8000/10000], D Loss: 1.0436799129820429e-05, G Loss: 12.742140769958496\n",
      "Epoch [8100/10000], D Loss: 1.2149485883128364e-05, G Loss: 12.20733642578125\n",
      "Epoch [8200/10000], D Loss: 1.1053915841330308e-05, G Loss: 12.367044448852539\n",
      "Epoch [8300/10000], D Loss: 1.1458818335086107e-05, G Loss: 12.202484130859375\n",
      "Epoch [8400/10000], D Loss: 1.0514731002331246e-05, G Loss: 12.35382080078125\n",
      "Epoch [8500/10000], D Loss: 9.192116522172e-06, G Loss: 12.606779098510742\n",
      "Epoch [8600/10000], D Loss: 9.083708391699474e-06, G Loss: 12.568814277648926\n",
      "Epoch [8700/10000], D Loss: 8.522401913069189e-06, G Loss: 12.629105567932129\n",
      "Epoch [8800/10000], D Loss: 8.445203093288e-06, G Loss: 12.581168174743652\n",
      "Epoch [8900/10000], D Loss: 8.316643288708292e-06, G Loss: 12.54970932006836\n",
      "Epoch [9000/10000], D Loss: 7.617589290020987e-06, G Loss: 12.688702583312988\n",
      "Epoch [9100/10000], D Loss: 7.331268079724396e-06, G Loss: 12.704363822937012\n",
      "Epoch [9200/10000], D Loss: 6.788765858800616e-06, G Loss: 12.85427474975586\n",
      "Epoch [9300/10000], D Loss: 6.654596290900372e-06, G Loss: 12.815143585205078\n",
      "Epoch [9400/10000], D Loss: 6.297025265666889e-06, G Loss: 12.860020637512207\n",
      "Epoch [9500/10000], D Loss: 5.878686351934448e-06, G Loss: 12.982101440429688\n",
      "Epoch [9600/10000], D Loss: 5.672735824191477e-06, G Loss: 12.9681396484375\n",
      "Epoch [9700/10000], D Loss: 4.457654085854301e-06, G Loss: 13.601651191711426\n",
      "Epoch [9800/10000], D Loss: 5.119162324263016e-06, G Loss: 13.113130569458008\n",
      "Epoch [9900/10000], D Loss: 4.783002623298671e-06, G Loss: 13.162755012512207\n",
      "Generated Protein Sequence: ECLMPTFCRFFFWFAR-RDHDLARATMMRI-TVDVP-SCHHHKEAMRWMYRDGGWCCALDEMLHWLFRKMSVGY-QALDQ-LGNFYPGIQPKNFCCG-PI\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义生成器（Generator）模型\n",
    "class ProteinSequenceGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, max_seq_len):\n",
    "        super(ProteinSequenceGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # 输入维度和嵌入维度\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 输出为氨基酸的数量（包括结束符）\n",
    "\n",
    "        self.max_seq_len = max_seq_len  # 生成序列的最大长度\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 将输入序列映射到低维空间\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
    "        out = self.fc(lstm_out)  # out shape: [batch_size, seq_len, output_dim]\n",
    "        return out\n",
    "\n",
    "    def generate_sequence(self, seed, max_seq_len):\n",
    "        \"\"\"\n",
    "        生成蛋白质序列\n",
    "        seed: 初始输入（通常是一个随机的氨基酸索引）\n",
    "        max_seq_len: 生成序列的最大长度\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            generated_sequence = seed  # 初始种子\n",
    "            for _ in range(max_seq_len - seed.size(1)):  # 生成最大序列长度\n",
    "                output = self.forward(generated_sequence)\n",
    "                next_idx = torch.argmax(output[:, -1, :], dim=-1)  # 选择概率最大的氨基酸\n",
    "                next_one_hot = torch.zeros(generated_sequence.size(0), 1, len(ALL_IDX)).scatter_(2, next_idx.unsqueeze(-1).unsqueeze(1), 1)\n",
    "                \n",
    "                # 调整维度使得可以拼接\n",
    "                generated_sequence = generated_sequence.unsqueeze(2)  # 将 generated_sequence 扩展为三维张量 [batch_size, seq_len, 1]\n",
    "                generated_sequence = torch.cat((generated_sequence, next_one_hot), dim=1)  # 在序列的末尾拼接 next_one_hot\n",
    "\n",
    "                # 如果生成的序列包含了 END_IDX，则停止生成\n",
    "                if next_idx.item() == END_IDX:\n",
    "                    break\n",
    "\n",
    "            return generated_sequence\n",
    "\n",
    "# 定义判别器（Discriminator）模型\n",
    "class ProteinSequenceDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(ProteinSequenceDiscriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # 输入维度和嵌入维度\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # 输出为真假判断\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 将输入序列映射到低维空间\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
    "        out = self.fc(lstm_out[:, -1, :])  # 只取最后一层输出进行分类\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# 定义生成器和判别器的优化器和损失函数\n",
    "generator = ProteinSequenceGenerator(len(ALL_IDX), embedding_dim=64, hidden_dim=128, output_dim=len(ALL_IDX), max_seq_len=100)\n",
    "discriminator = ProteinSequenceDiscriminator(len(ALL_IDX), embedding_dim=64, hidden_dim=128)\n",
    "\n",
    "criterion = nn.BCELoss()  # 二元交叉熵损失\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# 生成一个种子序列并将其转换为张量\n",
    "def amino_acid_to_tensor(sequence):\n",
    "    \"\"\"将氨基酸序列转换为模型输入的张量\"\"\"\n",
    "    tensor = torch.tensor([AA_TO_IDX.get(aa, END_IDX) for aa in sequence])  # 如果是 gap 使用 END_IDX\n",
    "    return tensor.unsqueeze(0)  # 返回一个 batch 的维度\n",
    "\n",
    "# 示例：训练生成器和判别器\n",
    "for epoch in range(10000):\n",
    "    # 生成真实数据（用一些实际的蛋白质序列）\n",
    "    real_sequence = amino_acid_to_tensor(\"M\" * 100)  # 示例的真实序列，假设有 100 个氨基酸\n",
    "    batch_size = real_sequence.size(0)\n",
    "    \n",
    "    # 生成假的数据（使用生成器）\n",
    "    noise = torch.randint(0, len(ALL_IDX), (batch_size, 100)).long()  # 随机噪声，种子为随机索引\n",
    "    fake_sequence = generator.generate_sequence(noise, max_seq_len=100)\n",
    "\n",
    "    # 判别器训练\n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    real_output = discriminator(real_sequence)\n",
    "    fake_output = discriminator(fake_sequence.detach())  # 不计算梯度\n",
    "    \n",
    "    real_label = torch.ones(batch_size, 1)\n",
    "    fake_label = torch.zeros(batch_size, 1)\n",
    "    \n",
    "    real_loss = criterion(real_output, real_label)\n",
    "    fake_loss = criterion(fake_output, fake_label)\n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "\n",
    "    # 生成器训练\n",
    "    optimizer_G.zero_grad()\n",
    "    \n",
    "    output = discriminator(fake_sequence)\n",
    "    g_loss = criterion(output, real_label)  # 目标是让生成的序列被判别器判为真实\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/10000], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "# 示例：生成一个蛋白质序列\n",
    "noise = torch.randint(0, len(ALL_IDX), (1, 96)).long()  # 随机生成一个种子\n",
    "generated_sequence = generator.generate_sequence(noise, max_seq_len=96)\n",
    "\n",
    "# 将生成的张量转换回氨基酸序列\n",
    "def tensor_to_amino_acid(tensor):\n",
    "    return ''.join([IDX_TO_AA[idx.item()] if idx.item() < len(AMINO_ACID_ALPHABET) else '-' for idx in tensor.squeeze()])\n",
    "\n",
    "generated_protein_sequence = tensor_to_amino_acid(generated_sequence)\n",
    "print(f\"Generated Protein Sequence: {generated_protein_sequence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Protein Sequence: SWYKWLIMVYAY-TGPLIPSH-SQ-NKSTMGEGFMQVQRMCVPV-KGYPFG-TYNPC-Y-QL-EIWFGLECFFLLQW-TD-CRQGRANGADGSCAI\n"
     ]
    }
   ],
   "source": [
    "noise = torch.randint(0, len(ALL_IDX), (1, 96)).long()  # 随机生成一个种子\n",
    "generated_sequence = generator.generate_sequence(noise, max_seq_len=96)\n",
    "\n",
    "# 将生成的张量转换回氨基酸序列\n",
    "def tensor_to_amino_acid(tensor):\n",
    "    return ''.join([IDX_TO_AA[idx.item()] if idx.item() < len(AMINO_ACID_ALPHABET) else '-' for idx in tensor.squeeze()])\n",
    "\n",
    "generated_protein_sequence = tensor_to_amino_acid(generated_sequence)\n",
    "print(f\"Generated Protein Sequence: {generated_protein_sequence}\")\n",
    "#生成一系列蛋白质序列，存储到Generated,faa文件中\n",
    "for i in range(1000):\n",
    "    noise = torch.randint(0, len(ALL_IDX), (1, 96)).long()  # 随机生成一个种子\n",
    "    generated_sequence = generator.generate_sequence(noise, max_seq_len=96)\n",
    "    generated_protein_sequence = tensor_to_amino_acid(generated_sequence)\n",
    "    with open('Generated.faa', 'a') as f:\n",
    "        f.write(f'>Generated_{i}\\n')\n",
    "        f.write(f'{generated_protein_sequence}\\n')\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
